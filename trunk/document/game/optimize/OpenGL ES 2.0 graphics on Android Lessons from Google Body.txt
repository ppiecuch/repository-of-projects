This is is the (expanded) transcript of a talk that I gave at GDC 2011. The final version of the code that I discuss can be seen at https://code.google.com/p/gdc2011-android-opengl/source/browse/trunk/src/com/example/gdc11/GDC11Activity.java . You can also look at the slides of the talk. 
Hi! I'm Nico, and I will talk a bit about writing OpenGL apps for Android. 
Earlier this year, I ported Google Body to Android. Users usually describe Google Body as "Google Earth for the human body". Let's take a quick look at it. You go to bodybrowser.googlelabs.com with a "modern browser" and wait a while for stuff to load. Then this model appears. You can rotate it, zoom in interactively, and so on. There's this transparency slider over here, where you can adjust body transparency. You can type something into the box up here, e.g. liver, and it'll show you where the liver is. You can click other things to get their names, too. 
Body is a webapp that uses a technology called WebGL to draw its 3d contents. WebGL is basically a javascript binding to OpenGL ES 2. There were two sessions about it yesterday. Currently, it's supported in Chrome and Firefox 4. It's not supported in Android's browser yet. Since Android does support ES 2, I hope this will change in the future, but at the moment it's not supported. 
At Google we have this concept of "20% time": A fifth of your week, you can work on whatever you want. Body was built by a few people on their 20% time, and they were looking for someone to port the product to Android. I thought it'd be fun to use body on a touch device, and I wanted to play with Android development for a while, so I signed up for this. 
I had hoped to find a good introduction on how to write OpenGL ES 2 apps on Android, but I couldn't really find one. In fact, the GL ES 2.0 sample in the Android SDK is even somewhat misleading. Hence, this talk. 
Let's take a quick look at how the Android port looks. As you can see, the model appears much faster due to being bundled with the app. There is no transparency slider on the left, instead there are layer selectors up here on the right. There's a little animation when you switch layers. There's also a search box with a fly-to animation. And you can tap entities to get their names. So that's what I did. 
As I said, I started working on the Android port early this year, and it's my first Android app. I have two months of Android experience, which probably isn't a ton. On the other hand, I can still remember all the things I struggled with when I started with this. And I do have some OpenGL experience from other projects. 
I'd like to point out that I'm not on the Android team. What I'm saying here is not the Android-sanctioned way of doing things, take it more as an experience report by a 3rd party app developer. 
With the disclaimers out of the way, let's get started, shall we? Out of interest, who here has OpenGL experience? OpenGL ES 2? Android? I think you will get the most out of this talk if you're fairly new to Android and maybe ES, but comfortable with OpenGL. 
Here's how to write high-performance applications on every platform in two easy steps: 
Measure performance. All the time. Start as soon as possible. Make sure it doesn't regress, keep notes about things that helped and that didn't. 
Follow best practices. In the case of Android and OpenGL, this includes things like "batch draw calls", "don't use discard in fragment shaders", and so on. 
This is my main point, the rest is details. I will return to them many times. 
Part 1: Hello OpenGL appI like hands-on talks, so we'll write a small demo app during this talk, I think this makes what I'm talking about "more real". All code I show will be available online after this talk at https://code.google.com/p/gdc2011-android-opengl/. I will show things that I learned while writing body, things I wished I had known earlier, and some other things that I believe are interesting to game developers, even though I don't have real first-hand experience with them. 
So, let's write an Android app! Here's a crash course, if you've never done this. You will need to download the Android 3.0 SDK. That's boring, so I won't show how to do it. Don't worry, you can write apps that run on older versions of Android with the 3.0 SDK. The Android folks recommend that you also download Eclipse and the Android plugin. I'm a vi guy myself, but when I'm new to something, I like to follow the recommended path until I'm comfortable with this first. I won't show how to download this either :-) 
Every Android app runs in a Java VM. Most Android apps are written in Java, but you can use other languages as well. There's also the so-called "NDK", which allows you to write your apps in C that's compiled to ARM directly, but NDK apps still run in a VM. Before 2.3, you had to write parts of the app in Java even if you used the NDK. I'll talk about the NDK later, it's interesting for game developers who want to run their code on several platforms, or who have really high performance demands. For just pushing pixels, Java bytecode is plenty fast though, since all you do is call OpenGL anyway. 
One central concept in an Android app is called an "Activity". An Activity is one screen of your app. It contains several "View"s, which represent stuff that's visible on screen. The Activity also gets notifications when it goes to the background, when the operating system is low on memory, and so on. 
Eclipse will create one Activity class for you when you use the Android plugin, and one Activity is all we need for most games where you usually want to get the system framework out of the way as fast as you can :-) Let's create a new project. 
(open page) Eclipse asks you a couple of questions, the interesting one being "Min SDK version". This is the lowest Android version your app. You put in a version code, and there's a table mapping version codes to the "normal" version number on the web. It's linked from http://developer.android.com/guide/appendix/api-levels.html (which you can find quickly by searching for "android version codes"). 
If you click on "Resources" up here and then "Platform Versions" on the left, you get a distribution of Android versions in use. About 90% are on 2.1 or higher, so 7 is the lowest lower bound you want to use in practice. If you want to write OpenGL ES 2 apps using Java, you want to require 2.2 because that version added ES2 java bindings. Having said that, ES2 was added to the NDK in 2.0 (api level 5), so you could use the NDK and/or create your own java bindings that you ship with your app if feel like can't target 2.2 or higher. If you look at the historical data down here, 2.2 is on a good path. 
All tablets run Android 3.0, i.e. api level 11. Google Body was targeted at tablets first, so it currently uses api level 11. For this example, let's go with 2.2, that is api level 8. 
(switch back to eclipse, create project) 
Let's see what Eclipse created for us. Here's the Activity subclass of our project. Remember, an Activity is "one screen" of the app. Another file that Eclipse generated is AndroidManifest.xml. The manifest file can contain all kinds of important metadata about your project, such as a list of activities - as you can see, our activity is listed here, as LAUNCHER activity. This means it will be opened when our application loads. 
There are basically two ways to test your app: By running it in an emulator, or on a real device. The emulator does not support OpenGL ES 2.0, so it's an easy choice. Running the app on an actual device is very easy: On the device, go to Settings->Applications->Development and enable "USB debugging". Now connect your device and hit the "Run" button in eclipse. Eclipse will then copy the apk file (show in bin folder in Finder) to the device and open it. 
So here's our app. (I'm showing this on a tablet because it has HDMI out.) Lines of code written so far: 0. Dialogs filled out so far: 1. Pretty easy. 
Now, let's get some accelerated pixels up this app. If you do a web search for 'android opengl', you will find this page, which claims that Android currently supports ES 1.0. This is wrong, it supports 2.0. It also has an "Using the API" section, which is also completely out-of-date. The linked example code is ok, yet incomplete. 
The way you should do OpenGL in Java is by using a GLSurfaceView. This class takes care of creating an OpenGL context for you, and it creates a renderer thread that all drawing is done on. You given the GLSurfaceView a "renderer" object, and the GLSurfaceView calls methods on the renderer object on the renderer thread. This is fairly easy to do, let me show you how. 
(Create GLSurfaceView and member variable, use Eclipse to create renderer class.) Beware: When you call any methods on a GLSurfaceView, Eclipse freezes for half a minute. This is why you never ever do work on your UI thread! 
Let's also override onPause() and onResume() to notify the gl view, so that it can stop and resume the render thread in these cases. 
This currently uses GL ES 1. This is the old fixed-function pipeline version. New apps should all use ES 2. Just tell the view that you want to use version 2 before you set the renderer, and it will take care of everything. (add setEGLContextClientVersion call). 
We should also add 
    <uses-feature android:glEsVersion="0x00020000" android:required="true" />to the manifest. This tells both the OS and the Android Market that the app will run only on devices that support ES 2.0 (which is most of them). 
Now if you run this, you get a black screen. We can change the fill color and add an explicit glClear to our renderer to change this color. To do this, we call static functions on the GLES20 class and ignore the GL10 object that's passed in to the renderer functions. (rename gl to unused). Oh, and if you don't like having GLES20. in front of every gl call, you can add 
  import static android.opengl.GLES20.*;then it's no longer needed. As you've seen, onDrawFrame is called on every frame. onSurfaceCreated is called when the gl context is created, and onSurfaceChanged is called when the surface size changes (think rotation). 
Gray screen, yay! This is how you get accelerated pixels on screen in Android. Lines written so far: ~50. And we have a dedicated render thread. Not too bad. 
Remember the two points on how to write an efficient app? First was "always measure performance". Let's add code for that right now. We'll just add some very basic code that prints the current ms per frame for the last 50 frames every 50 frames. 
  ++mFrameCount;   if (mFrameCount % 50 == 0) {       long now = System.nanoTime();       double msPerFrame = (now - mStartTimeNS) / 1e6 / mFrameCount;       Log.i(TAG, "ms per frame: " + msPerFrame +               " (fps: " + (1000/msPerFrame) + ")");       mFrameCount = 0;       mStartTimeNS = now;   }Simple, but it works for now. The "i" means "info log". There's also "w" for warnings, and so on. You can use 
  adb logcatto watch the device's logs. 60 fps on the tablet, likely 60 fps on phones. Use 
  adb logcat MyApp:V *:Wto see all log output of your app and only warnings or higher of the system. Note that if your app crashes, the offending exception isn't printed on your app's log but on AndroidRuntime's, so don't look at just your app's output. adb is in the platform-tools directory of the sdk. It's short for "Android debug bridge", and you can use this to pull all kinds of data from your device. 
As you can see, the app logs output continuously. GLSurfaceView is in continuous mode by default, where it calls your renderer's onDrawFrame() as often as it can, up to 60fps. You can also put it in on-demand mode by calling 
  setRenderMode(GLSurfaceView.RENDERMODE_WHEN_DIRTY)Then it's only drawn when someone calls 
  requestRender()on it. For games, you probably want continuous rendering, but Body is in on-demand mode most of the time: While you interact with it, it's in continuous mode, and after 1 second after the last interaction, when all animations are done, it switches to on-demand mode to safe battery. 
Best practice: Use GLSurfaceView to create OpenGL contexts. 
Even if you use the ndk, you can use GLSurfaceView to create a render thread for you. You'd then just forward the three renderer calls to your native code. 
If the default behavior of GLSurfaceView is not good enough, chances are that it can be configured to do what you want, see the various setEGLConfigChooser methods -- more on this below. Note that GLSurfaceView creates 16 bit color buffers by default, but that's usually what you want (XXX: dangerous config behavior) If you're curious how GLSurfaceView implements something, you can always look at its source. One way is by searching for "GLSurfaceView package:android" on codesearch.google.com. 
The renderer's onSurfaceCreated() method is actually called pretty often, because GLSurfaceView recreates its renderer every time it's unpaused. So your code that uploads data to OpenGL is better fast. If you're on api level 11, you can call setPreserveEGLContextOnPause() to hint that you'd rather have your context kept around, but if the device supports just one GL context at a time, you will have to reupload data on activate anyway. 
One way to make sure this is fast is to keep the ready-to-upload data around in RAM. When your activity's onLowMemory() is called, drop these caches. 
(open body) Body has this search box up here, which is implemented using Android's search framework. In an early version, typing in a search caused the context to go away, and everything had to be reloaded. On Honeycomb (Android 3.0), one can intercept search requests, so that's what I'm currently doing, but for the phone version I plan to keep the decoded data round in RAM and then do a quick reupload. 
One case where reloading happens by default is when the screen rotates. What happens is that Android throws away your whole activity on screen rotation and creates a new, rotated one by default.You can fix this particular common case by adding 
  android:configChanges="orientation"to your activity in the manifest. If your app shouldn't change its orientation, use 
  android:screenOrientation="landscape"instead. This used to be janky in Body as well. I can't demo how nice this works now because of the HDMI output. 
Part 2: Resource loadingThere are two different approaches to bundle resources with your app: 
Bundle them with the apk 
Download all resources from the web to the sdcard on first run 
The tradeoffs are mostly obvious: Bundling is a lot easier: The apk is self-contained, and it's easy to access the bundled files from code. But it means that all resources need to be re-downloaded on every application update. Also, only apks up to 50MB can be uploaded to the Android market. 
APKs are installed to internal memory, and that's limited on many phones. On Android 2.2 or higher, you can add 
  android:installLocation="auto"to your manifest to allow Android to keep your apk on the sdcard. 
Body bundles everything with the apk, mostly because we didn't have headcount to implement the "download everything on first run" option. If you don't have a lot of data, say 5-10 MB, this is probably the way to go. This is probably true for most casual games. 
There are two ways to bundle resources with your apk: 
Copy them to a subdirectory of the "res" folder of your project 
Copy them to the "assets" folder of your project 
The only real difference between 1 and 2 is that all files in "res" get a unique identifier assigned by the build system, and you open them by this ID, while you open files in "assets" by their filename. 
"res" has many specific subfolders for localized text, UI layouts, etc. See http://developer.android.com/guide/topics/resources/providing-resources.html for the full list. For games the two most interesting subfolders are "raw", where arbitrary binary files go, and "drawable", where images go. Note that the icon is in folders called "drawable-hdpi", "-lmdpi", and so on. Android will then automatically pick the icon that's the right resolution for the current screen. You probably don't want this to happen for your textures -- you can put them into drawable-nodpi, where it won't happen. 
Here's how you open an InputStream to a resource (in your activity): 
  InputStream is = getResources().openRawResource(R.raw.my_resource);By the way, filenames in res/ have to look like Java identifiers. Here's the same for a resource stored in "assets": 
  InputStream is = getAssets.open("myfile.foo");  // Omit "assets/".By the way, after you added files to your "res" or "assets" folders, be sure to right-click that folder in Eclipse and choose "Refresh" to make sure the new files get added to your apk. 
Before 2.3, you couldn't open resources from native code, you had to open the files from Java (or from C using JNI). Since 2.3, there's AAssetManager_fromJava and the functions in android/assert_manager.h. 
Best practice: Make sure your bundled resources are in a format that doesn't need any conversions at load time. Example: If your images need to be flipped, do this before you bundle them. 
The graphics data you want to bundle with your app are 
meshes: attributes and indices 
textures 

MeshesLet's look at meshes first. By far the best way to draw (static) meshes using OpenGL ES 2 is to use vertex buffer objects (vbos). A vbo is just a chunk of graphics memory that contains either attribute or index data. To render data, you can then say "draw this subrange of this vbo", and then everything happens on the gpu, so everything's fast. 
Best practice: Use VBOs for everything. (The GLES20 example in the Android SDK doesn't.) 
You need to upload your asset data to an vbo once during application startup. How can you do this as quickly as possible? The GLES20 function that uploads data into the currently bound vbo is: 
  void glBufferData(int target, int size, Buffer data, int usage)The interesting parameter is data, which is a java nio Buffer. These buffers come in two flavors: Backed by a regular java array, or backed by a raw memory region -- think something returned by malloc. The latter are called direct buffers. A direct buffer is allocated using ByteBuffer.allocateDirect(), and glBufferData() wants a direct byte buffer. 
The naive way to load a resource into a direct buffer looks like this: 
  // DO NOT DO THIS.   InputStream is = getResources().openRawResource(R.raw.myresource);   int size = is.available(), v; // Note: Guaranteed to return whole size   ByteBuffer b = ByteBuffer.allocateDirect(size);   while ((v = is.read()) != -1)     b.put(v);   // A buffer maintains an internal "current index"   b.rewind();This is unsurprisingly very slow. This is better, but still slow: 
  // DO NOT DO THIS.   byte[] buff = new byte[size];  // In a real app, use a block size here.   is.read(buff);   for (int i = 0; i < size; ++i)     b.put(buff[i]);The reason for this is that accessing a single element of a direct byte buffer is surprisingly slow: It's not only one virtual method call per element, it's also one JNI hop, since direct byte buffers are implemented in C. Try to never set single elements of byte buffers in a java loop. This version works well: 
  byte[] buff = new byte[size];  // In a real app, use a block size here.   is.read(buff);   b.put(buff);It's over 10x faster than the first version. 
There's another way that's faster still, but it can't always be used. Resources are normally compressed, which makes them quite a bit smaller. However, the build system keeps a set of magic extensions where compression is usually not helpful, and hence these files are not compressed. Examples are jpg and png files, but also a bunch of others. I usually use ".jet" for files that I don't want to have compressed. So if you name your file "myfile.jet", it won't be compressed. This is true for files in "res" and "assets" alike. Uncompressed files can be opened as real FileInputStreams. For these files, you can use this code to get a direct byte buffer with the contents: 
  AssetFileDescriptor ad = getResources().openRawResourceFd(R.raw.myfile);       // or getAssets().openFd("myfile.jet")   FileInputStream fis = ad.createInputStream();   FileChannel fc = fis.getChannel();   MappedByteBuffer b = fc.map(  // Guaranteed to be direct.       MapMode.READ_ONLY, ad.getStartOffset(), ad.getLength());   long size = ad.getLength();   // upload b here   fc.close();   fis.close();This uses Java's nio Channel class. This way, the data never has to be copied into the java vm and can stay in direct memory all the time. This is another 10x faster. If you can't use this method because you need compression, the previous one is ok. If you need to do any processing of your input data, consider doing this in native code. 
Body currently uses the same resources that the web version uses. Since the web version downloads all asset files at startup, it uses some fancy custom compresssion scheme for its geometry. The decompression code in Body for Android was very slow. It got much faster when I stopped doing the decompression on direct byte buffers and instead copied chunks of data into java arrays, then did the decompression logic there, and then copied that back. In the future, I'd like to stick to the "don't do complicated things at startup" best practice I mentioned above. 
Best practice: Never ever do I/O on the UI thread. Not on the render thread, either. 
I/O can be slow. If this slowness is on the UI thread, your app will be unresponsive during that time. If it's on the render thread, the app won't paint. Both are bad for the user -- always do I/O on a dedicated I/O thread, and use callbacks and Android's Handler class to let the I/O thread notify other threads when things have finished loading. 
(show how Body loads) Even though Body still loads somewhat slowly, since it happens in the background I don't feel too bad about shipping this as a first version. 
Let's add this to our little demo app. I prepared a file that contains a bunch of vertices -- 60000 of them, with position, normal, and one texture coordinate per vertex. Every attribute is a float; in real life, you'd probably use floats for the position, signed bytes for the normal, and shorts for the texture coordinates - but measure performance and checks what works best for you. (add loadResourceToBuffer() function, mVertexBuffer) 
I also prepared a list of tristrip indices to connect these vertices, let's load these, too. (mTristripBuffer, mNumTristripIndices) 
GL ES 2.0 does no longer have a fixed pipeline, you have to use shaders. Compiling shaders is a bit verbose, so let me just paste in the code for that. (paste in shader code) You can see the shaders are very simple. We bind position, normal, and texture coordinate to indices 0, 1, and 2 respectively. The vertex shader just copies data to the fragment shader, and the fragment shader writes a solid color for now. 
To draw a VBO, we bind the shader. Then we bind the vertex VBO and tell OpenGL, which parts of it belong to which attribute. Attribute 0, position, is three floats at offset 0, and the distance to the next position is 32 byte. 
There are two versions of glVertexAttribPointer(), the one that takes an offset as last parameter, and one that takes a Buffer as last parameter. Do NOT use the latter version, it reuploads the data on every call! This is a lot slower - you wouldn't reupload texture data on every frame either. Sadly, this is what the ES20 example in the Android SDK uses. 
Oh, and there's another problem: On Android 2.2, the binding to the glVertexAttribPointer() overload that takes the offset is missing -- your program will compile just fine, but it will crash with an "unresolved linker" error at runtime when you call this function. This gives me a good excuse to show you how this JNI stuff that I mentioned a few times already works. 
First, you add methods with the "native" keyword to a class: 
    native public static void glDrawElements(             int mode, int count, int type, int offset);     native public static void glVertexAttribPointer(             int index, int size, int type,             boolean normalized, int stride, int offset);Then you create a folder "jni", and put the C implementation of these two methods there. We will give the functions special names, and JNI will find them by name lookup. In a real app, you'd probably want to implement JNI_OnLoad() instead and register them explicitly; that's a bit faster. See LINK for details. 
Then download the NDK, and in the jni folder run "ndk-build". Do a clean build in Eclipse to get your so file bundled, and you're all set. Now we can call our fixed version of glVertexAttribPointer() to draw geometry. 
This is how you can load geometry very very quickly. The original code took over a second to load and upload all geometry, now it takes under 2 hundreds of a second. 
Textures next! 
TexturesBest practice: Use ETC for texture compression. 
All Android handsets that support OpenGL ES 2 also support ETC texture compression. Compressed textures need less RAM and render faster. 
ETC-compressed textures are effectively 4 bit per pixel, so a 1k x 1k texture needs 512kB, compared to 2MB it needs with rgb565 pixels. 
ETC supports only rgb textures; if you need an alpha channel, you can't use it. In this case, you can use low-resolution uncompressed textures. Every GPU supports a few GPU-specific compression methods as well - if you can, provide textures in the compressed formats supported by the nVidia Tegra2 chips (dxt), Imagination Technologies's POWERVR (PVRTC), and Qualcomm's Adreno (ATITC). See http://developer.motorola.com/docstools/library/understanding-texture-compression/ 
But rgb textures are useful for a wide area of applications, and you want to use ETC in these cases. ETC data is stored in a very simple container format called "pkm" on Android. Remember that Android compresses most resources; it does compress pkm files. Even though this is double compression, the file size does go down by the second compression. A compressed pkm file is usually about the same size as a jpeg file, but it loads and renders faster. 
There's not a lot of documentation on how to create pkm files on the web. Ericsson offers an open-source tool called "etcpack" for download. It's precompiled for windows, and it builds with minor modifications on Unix systems as well. However, it can only convert ppm files. You can use ImageMagick to convert your assets to ppm, and then etcpack to convert to pkm: 
# This is on OS X export DYLD_LIBRARY_PATH=/path/to/ImageMagick-6.6.5/lib/ /path/to/ImageMagick-6.6.5/bin/convert mytexture.jpg mytexture.ppm /path/to/etcpack/etcpack mytexture.ppm res/raw/mytexture.pkmYou can call etcpack with -h to get a list of options. 
To create mipmaps, you can then do something like this: 
for level in {1..11}; do   /path/to/ImageMagick-6.6.5/bin/mogrify -resize 50% mytexture.ppm   /path/to/etcpack/etcpack mytexture.ppm res/raw/earth_map_$level.pkm doneYou can upload this to the current texture like so (requires 2.2): 
  ETC1Util.loadTexture(GLES20.GL_TEXTURE_2D, 0, 0,       GLES20.GL_RGB, GLES20.GL_UNSIGNED_SHORT_5_6_5,       getResources().openRawResource(R.raw.mytexture));In a real app, you'd want to use ETC1Util.createTexture() to load the resource on the I/O thread, and then just use the overload of ETC1Util.loadTexture() that takes the result of createTexture() on the renderer thread. 
Let's quickly add a texture to the demo app. (do so -- no mipmapping) 
Just for completeness, this is how you load a non-compressed texture: 
  BitmapFactory.Options opts = new BitmapFactory.Options();   opts.inScaled = false;   opts.inPreferredConfig = Bitmap.Config.RGB_565;   Bitmap bitmap = BitmapFactory.decodeResource(       getResources(), R.drawable.my_jpg_texture, opts);   GLUtils.texImage2D(GLES20.GL_TEXTURE_2D, 0, bitmap, 0);   bitmap.recycle();The inScaled parameter prevents that Android changes the pixel dimensions of the texture based on the display resolution. That would probably break the power-of-two size of the texture, so we don't want that. Alternatively, you can put your drawables into res/drawable-nodpi, then they won't be resized either. 
...and that's how you can load assets pretty quickly. 
Body as of now uses jpegs as textures that are converted to 16bpp pixels at load time. That's mostly because I didn't know how pkm files worked back then. Now that I found out, I will update it to use compressed textures soon. 
Best practice: Make sure your geometry and texture resolutions are appropriate for the size they're displayed at. Don't use a 1k x 1k texture for something that's at most 500 pixels wide on screen. The same for geometry. 
Links for JNI section: 
http://java.sun.com/docs/books/jni/download/jni.pdf 
http://download.oracle.com/javase/1.4.2/docs/guide/jni/jni-14.html 
http://android.git.kernel.org/?p=platform/dalvik.git;a=blob_plain;f=docs/jni-tips.html;hb=HEAD 

Part 3: Drawing performanceThis is mostly a laundry list of things I've either done in Body, or of things that I want to try in the future. All the GPU hardware manufacturers have guides with tips on how to get the best out of their hardware. When you experiment with these things, make sure to measure performance and to keep good notes. 
It's useful to have a rough mental model of how GPUs work, and where the bottlenecks can be. My mental model: 
Video memory (which is usually shared with normal RAM on mobile devices) contains vertex data and texture data. When geometry is drawn, it's sent to the vertex shading units. These units can have a pre-transform cache and a post-transform cache. The post-transform cache caches the pixel shader output and it can be tiny -- it's 2 vertices on the Adreno chipset for example. These caches are usually basic fifo caches. The Adreno manages to cache tristrips with this, but not much more. The output of the vertex shader is then rasterized and sent to the fragment shader, which reads this and texture data from video memory and produces an output color that's written to the framebuffer. 
To make all these caches happy, try to have good cache coherency. 
Basic guidelines: 
use vbos always 
use indices 
use interleaved vertices 
use "small" attributes: signed bytes for normals might be enough, halfs for texture coordinates might be enough (GL_OES_vertex_half_float is widely supported) 

This helps with cache behavior. 
If the vertex shader is the bottleneck, you can improve things by using smaller or fewer attributes, fewer varyings, smaller varyings (OpenGL ES has this precision keyword), or LOD. Using indexed geometry helps too, but you should always do that. 
If the fragment shader is the bottleneck, try to move code from the fragment shader to the vertex shader. If you're doing expensive calculations in the fragment shader, you could try replacing them with texture lookups instead. 
To make sure the gpu doesn't have to wait for the gpu, submit geometry in batches. You can for example combine several triangle strips into one by connecting them with degenerate triangles. You can use texture atlases to be able to put more objects into a single glDrawElements call. 
This also reduces the number of state changes, which is important, too. The most expensive state change is usually a shader change, so sort your objects by shader first, and texture second. 
Keep your shaders short, a single line can make a huge difference. I improved body performance by 17% just by removing a normalize() call from the vertex shader. You're generally fill-bound, so try to move things from the fragment shader to the vertex shader, so try to move things from the fragment shader to the vertex shader. Instead of transforming normals and light to world space, you can transform the light vector to model space on the cpu per model and then do lighting in model space in the shader. 
Use mipmaps, that helps with texture fetch performance and looks nicer if objects show up at different depths. This helps with the texture cache. Also, use texture compression. 
Don't use discard in fragment shaders if you can avoid it. 
Do high-level object culling on the CPU. 
Keep device variance in mind: Most phones that support OpenGL ES 2 have roughly comparable hardware, but the tablets are much faster. 
GL spec: http://www.khronos.org/registry/gles/specs/2.0/es_cm_spec_2.0.24.pdf 
Device manufacturer guides: http://developer.download.nvidia.com/tegra/docs/tegra_gles2_development.pdf http://www.imgtec.com/factsheets/SDK/POWERVR%20SGX.OpenGL%20ES%202.0%20Application%20Development%20Recommendations.1.1f.External.pdf http://developer.qualcomm.com/forum/android/graphics-optimization/467 (you need to register to download) 
Bonus part: Touch inputWhile not directly related to graphics, I'd like to very quickly describe a way to handle touch input. The basics are very simple: You just override a view's onTouchEvent(), which receives raw touch events. Pass this on to GestureDetector for scroll and tap events and to ScaleGestureDetector for zoom events, and you're almost all set: 
    @Override     public boolean onTouchEvent(final MotionEvent event) {         mScaleDetector.onTouchEvent(event);         mTapDetector.onTouchEvent(event);         return true;     }This goes into a GLSurfaceView subclass. 
One issue is that you'll receive spurious onSingleTapUp() events while two-finger-zooming. A simple and effective solution is to have a short dead time after two-finger zooming and one-finger scrolling, and to check for this in onSingleTapUp(). If the tap is too soon, just drop it. 
You can use the view's queueEvent() method to queue a Runnable to run on the renderer thread: 
  public boolean onScroll(MotionEvent e1, MotionEvent e2,           final float dx, final float dy) {       queueEvent(new Runnable(){               public void run() {                   mRenderer.drag(dx, dy);               }});       mLastNonTapTouchEventTimeNS = System.nanoTime();       return true;   }Likewise, the renderer can use the view's Handler to post a message back to the UI thread. 
In reaction to touch input, the demo app rotates and dollies the view. 
If you'd like to find the object the user touched, a good way to do this is by rendering the scene into an offscreen surface, with a unique color per object. Disable blending, multisampling, and everything that changes colors. Then check the color of the pixel below the tap point (and a small neighborhood, if nothing is at that pixel), and map from the color to the corresponding object. 
Bonus part: MultisamplingIf you have frames to burn (unlikely), consider turning on multisampling - it considerably improves the visual quality of your app. To do this, you need to set a custom EGLConfigChooser on the GLSurfaceView before you set the renderer. In the chooser, you will then select configs that have EGL_SAMPLE_BUFFERS set to 1 ("true") and EGL_SAMPLES to something greater than 1, e.g. 2. 
The Tegra chips do not support normal multisampling, but they do support a vendor-specific technology called "coverage sampling". If no configs with EGL_SAMPLE_BUFFERS are found, search for EGL_COVERAGE_BUFFERS_NV == 1 and EGL_COVERAGE_SAMPLES_NV > 1 and use one of these configs to get multisampling on the Tegra chips. 
Be careful: eglChooseConfig() returns the color-deepest buffers first, and it returns all configs as good or better than the requested attributes. So if you request r=5, g=6, b=5, the first elements in the returned array will likely have r=8, g=8, b=8. So don't just pick the first element of the results that eglChooseConfig() returns! 
Closing thoughts: 
Look at Android code! 
but don't depend on internal stuff 
use traceview and ddms 

 


Comment by LucianoM...@gmail.com, Jul 28, 2011 
You have a example for find the object the user touched? 

Comment by chenghsi...@gmail.com, Feb 14, 2012 
Hi,nico: I want to consult you that how to create a .jet file? Does any reference for this extension? thank you for help 

Comment by DSu...@gmail.com, May 29, 2012 
Hi, 

I can't understand why globe has wrong shape like is ellipsoid instead sphere? Can anyone explain me how get right shape for this example? 

Comment by zerd...@gmail.com, Apr 4, 2013 
Hi! Thanks for this awesome information, I learned a lot! Regarding .jet file - you can put any data in there, extension is just for this file was not compressed. I put raw integers one by one - these are my vertices. 



http://code.google.com/p/gdc2011-android-opengl/wiki/TalkTranscript