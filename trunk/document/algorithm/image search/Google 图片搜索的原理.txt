针对这个问题，请教了算法组的同事，他分享了基本的思路：

对于这种图像搜索的算法，一般是三个步骤：

1. 将目标图片进行特征提取，描述图像的算法很多，用的比较多的是：SIFT描述子，指纹算法函数，bundling features算法，hash function(散列函数)等。也可以根据不同的图像，设计不同的算法，比如图像局部N阶矩的方法提取图像特征。

2. 将图像特征信息进行编码，并将海量图像编码做查找表。对于目标图像，可以对分辨率较大的图像进行降采样，减少运算量后在进行图像特征提取和编码处理。

3. 相似度匹配运算：利用目标图像的编码值，在图像搜索引擎中的图像数据库进行全局或是局部的相似度计算;根据所需要的鲁棒性，设定阈值，然后将相似度高的图片预保留下来;最后应该还有一步筛选最佳匹配图片，这个应该还是用到特征检测算法。

其中每个步骤都有很多算法研究，围绕数学，统计学，图像编码，信号处理等理论进行研究。

下面是阮一峰的一个最简单的实现：

你输入Google图片的网址，或者直接上传图片，Google就会找出与其相似的图片。下面这张图片是美国女演员Alyson Hannigan。



上传后，Google返回如下结果：



这种技术的原理是什么?计算机怎么知道两张图片相似呢?

根据Neal Krawetz博士的解释，原理非常简单易懂。我们可以用一个快速算法，就达到基本的效果。

这里的关键技术叫做"感知哈希算法"(Perceptual hash algorithm)，它的作用是对每张图片生成一个"指纹"(fingerprint)字符串，然后比较不同图片的指纹。结果越接近，就说明图片越相似。

下面是一个最简单的实现：

第一步，缩小尺寸。

将图片缩小到8x8的尺寸，总共64个像素。这一步的作用是去除图片的细节，只保留结构、明暗等基本信息，摒弃不同尺寸、比例带来的图片差异。



第二步，简化色彩。

将缩小后的图片，转为64级灰度。也就是说，所有像素点总共只有64种颜色。

第三步，计算平均值。

计算所有64个像素的灰度平均值。

第四步，比较像素的灰度。

将每个像素的灰度，与平均值进行比较。大于或等于平均值，记为1;小于平均值，记为0。

第五步，计算哈希值。

将上一步的比较结果，组合在一起，就构成了一个64位的整数，这就是这张图片的指纹。组合的次序并不重要，只要保证所有图片都采用同样次序就行了。



得到指纹以后，就可以对比不同的图片，看看64位中有多少位是不一样的。在理论上，这等同于计算"汉明距离"(Hamming distance)。如果不相同的数据位不超过5，就说明两张图片很相似;如果大于10，就说明这是两张不同的图片。

具体的代码实现，可以参见Wote用python语言写的imgHash.py。代码很短，只有53行。使用的时候，第一个参数是基准图片，第二个参数是用来比较的其他图片所在的目录，返回结果是两张图片之间不相同的数据位数量(汉明距离)。

这种算法的优点是简单快速，不受图片大小缩放的影响，缺点是图片的内容不能变更。如果在图片上加几个文字，它就认不出来了。所以，它的最佳用途是根据缩略图，找出原图。

实际应用中，往往采用更强大的pHash算法和SIFT算法，它们能够识别图片的变形。只要变形程度不超过25%，它们就能匹配原图。这些算法虽然更复杂，但是原理与上面的简便算法是一样的，就是先将图片转化成Hash字符串，然后再进行比较。


http://www.oschina.net/news/29402/google-pic-search